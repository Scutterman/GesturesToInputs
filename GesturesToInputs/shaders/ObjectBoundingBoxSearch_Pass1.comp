#version 430 core

struct ObjectSearchData {
    uvec4 boundingBox; // minimum left and minimum top, maximum right and maximum bottom of all contiguous areas
    uint area; // Number of pixels inside the bounding box
    uint topLeftSampleIndex;
    uint objectTrackerIndex;
    uint isObjectTopLeft;
};

struct TrackerData {
    vec4 colour;
    uint horizontalPosition;
    uint verticalPosition;
    uint orientation;
    uint isVisibleInFrame;
};

layout(std430, binding=4) buffer data {
    ObjectSearchData samples[];
};

layout(std430, binding=6) buffer data2 {
    TrackerData trackers[];
};

layout(local_size_x=1, local_size_y=1, local_size_z=1) in;

uniform layout(binding=1, rgba32f) image2D thresholdTexture;
uniform layout(binding=2, rgba32f) image2D outputImage;
uniform ivec2 samplePixelDimensions; // width and height of each sample in pixels - larger sample size is faster but less accurate
uniform ivec2 sampleSize; // number of horizontal and vertical samples in the image
uniform uint numberOfPasses; // Higher number of passes = higher chance at successfully working out the bounding box but higher processing time. May be possible to best-guess on CPU based on number of sample rows and columns
uniform uint threshold; // If a bounding box contains fewer than this many pixels, the object is discounted. Higher = less work for the next stage but may discount valid markers.

// Propogate the lowest top/left and the highest right/bottom values between two samples
void atomicBoundingBoxUpdate(uint currentSample, uint targetSample) {
    atomicMin(samples[currentSample].boundingBox.x, samples[targetSample].boundingBox.x);
    atomicMin(samples[currentSample].boundingBox.y, samples[targetSample].boundingBox.y);
    atomicMax(samples[currentSample].boundingBox.z, samples[targetSample].boundingBox.z);
    atomicMax(samples[currentSample].boundingBox.w, samples[targetSample].boundingBox.w);
    atomicMin(samples[targetSample].boundingBox.x, samples[currentSample].boundingBox.x);
    atomicMin(samples[targetSample].boundingBox.y, samples[currentSample].boundingBox.y);
    atomicMax(samples[targetSample].boundingBox.z, samples[currentSample].boundingBox.z);
    atomicMax(samples[targetSample].boundingBox.w, samples[currentSample].boundingBox.w);
}

vec3 hsv2rgb(vec3 hsv)
{
  vec4 K = vec4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0);
  vec3 p = abs(fract(hsv.xxx + K.xyz) * 6.0 - K.www);
  return vec3(hsv.z * mix(K.xxx, clamp(p - K.xxx, 0.0, 1.0), hsv.y));
}

void main()
{
    ivec2 imageDimensions = imageSize(thresholdTexture);
    uint sampleColumn = gl_WorkGroupID.x;
    uint sampleRow = gl_WorkGroupID.y;
    uint sampleIndex = sampleColumn + (sampleRow * sampleSize.x);
    uint tracker = gl_WorkGroupID.z;
    
    // Reset sample data
    samples[sampleIndex].isObjectTopLeft = 0;
    samples[sampleIndex].objectTrackerIndex = -1; // this should be UINT_MAX
    samples[sampleIndex].area = 0;
    samples[sampleIndex].topLeftSampleIndex = sampleIndex;
    samples[sampleIndex].boundingBox = ivec4(sampleColumn, sampleRow, sampleColumn+1, sampleRow+1);

    memoryBarrier();

    vec4 normalisedTrackerColour = vec4(trackers[tracker].colour.x / 179, trackers[tracker].colour.y / 255, trackers[tracker].colour.z / 255, trackers[tracker].colour.w / 255);
        
    // Determine whether sample has enough pixels to be part of an object
    uint balance = 0;
    uint target = (samplePixelDimensions.x * samplePixelDimensions.y) / 2;
    for (uint j = 0; j < samplePixelDimensions.y; j++) {
        for (uint i = 0; i < samplePixelDimensions.x; i++) {
            uint x = min((sampleColumn * samplePixelDimensions.x) + i, imageDimensions.x);
            uint y = min((sampleRow * samplePixelDimensions.x) + j, imageDimensions.y);
            vec4 pixel = imageLoad(thresholdTexture, ivec2(x, y));
            balance += pixel.x == normalisedTrackerColour.x && pixel.y == normalisedTrackerColour.y && pixel.z == normalisedTrackerColour.z ? 1 : 0;
        }
    }

    bool hasSufficientMatchingPixels = (balance >= target);
    if (hasSufficientMatchingPixels) { samples[sampleIndex].objectTrackerIndex = tracker; }
    memoryBarrier();

    // Susurration - lowest values for top & left / highest values for right & bottom will propogate through the whole object
    // Only need to check certain directions because other samples will update our values for the other direction when they make their checks
    for (uint i = 0; i < numberOfPasses; i++) {
        int leftIndex = int(sampleIndex) - 1;
        if (hasSufficientMatchingPixels && leftIndex > 0 && samples[leftIndex].objectTrackerIndex == tracker) {
            atomicBoundingBoxUpdate(sampleIndex, leftIndex);
        }
        
        int topIndex = int(sampleIndex) - sampleSize.x;
        if (hasSufficientMatchingPixels && topIndex > 0 && samples[topIndex].objectTrackerIndex == tracker) {
            atomicBoundingBoxUpdate(sampleIndex, topIndex);
        }
        
        int topLeftIndex = (int(sampleIndex) - sampleSize.x) - 1;
        if (hasSufficientMatchingPixels && topLeftIndex > 0 && samples[topLeftIndex].objectTrackerIndex == tracker) {
            atomicBoundingBoxUpdate(sampleIndex, topLeftIndex);
        }
        
        int bottomLeftIndex = (int(sampleIndex) + sampleSize.x) - 1;
        if (hasSufficientMatchingPixels && bottomLeftIndex > 0 && samples[bottomLeftIndex].objectTrackerIndex == tracker) {
            atomicBoundingBoxUpdate(sampleIndex, bottomLeftIndex);
        }
        
        // Unfortunately need to wait and make sure all samples have finished their checks this pass before the next iteration
        memoryBarrier();
    }

    // Update this sample's information with the propogated object values
    uint topLeftIndex = samples[sampleIndex].boundingBox.x + (samples[sampleIndex].boundingBox.y * sampleSize.x);
    samples[sampleIndex].topLeftSampleIndex = topLeftIndex;
    
    ivec2 topLeft = ivec2(samples[sampleIndex].boundingBox.x * samplePixelDimensions.x, samples[sampleIndex].boundingBox.y * samplePixelDimensions.y);
    ivec2 bottomRight = ivec2(samples[sampleIndex].boundingBox.z * samplePixelDimensions.x, samples[sampleIndex].boundingBox.w * samplePixelDimensions.y);
    uint width = bottomRight.x - topLeft.x;
    uint height = bottomRight.y - topLeft.y;
    
    uint area = width * height;
    if (hasSufficientMatchingPixels && area < threshold) {
        samples[sampleIndex].area = 0;
        samples[sampleIndex].objectTrackerIndex = -1; // Should be UINT_MAX
    } else if (hasSufficientMatchingPixels) {
        samples[sampleIndex].area = area;
    }

    memoryBarrier();

    // Only store the final values in one sample so it's quicker to isolate in the next stage
    if (samples[sampleIndex].objectTrackerIndex == tracker) {
        samples[topLeftIndex].isObjectTopLeft = 1;
        samples[topLeftIndex].objectTrackerIndex = tracker; // Assume only one tracker has this sample as its top-left corner.
        samples[topLeftIndex].area = area; // This could store an exact number of pixels in the object using the "balance" variable and an atomic add.
        samples[topLeftIndex].boundingBox = samples[sampleIndex].boundingBox;

        vec4 outputColour = vec4(hsv2rgb(normalisedTrackerColour.xyz), 1);
        for (uint i = sampleColumn * samplePixelDimensions.x; i < (sampleColumn + 1) * samplePixelDimensions.x; i++) {
            for (uint j = sampleRow * samplePixelDimensions.y; j < (sampleRow + 1) * samplePixelDimensions.y; j++) {
                imageStore(outputImage, ivec2(i,j), outputColour);
            }
        }
    }
}
